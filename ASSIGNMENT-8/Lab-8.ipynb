{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7cccbcd-aff7-4471-969a-000a2e960237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02225f6b-ff5c-428d-b541-4f38fe92f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    # 1. Handle zero-width joiner\n",
    "    sentence = re.sub(\"\\u200c\", \" \", sentence)\n",
    "\n",
    "    # 2. Replace URLs (http, https, www)\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', sentence)\n",
    "\n",
    "    # 3. Replace numbers (any continuous digits)\n",
    "    sentence = re.sub(r'\\d+', '<NUMBER>', sentence)\n",
    "\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' <PUNCT> ', sentence)\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa32bb4b-3cfa-4f94-bbf4-e01d3e8f3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_with_normalization(sentence, smoothing=False):\n",
    "    TF = {}\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            TF[word] += 1\n",
    "        except:\n",
    "            TF[word] = 1\n",
    "\n",
    "    length = len(sentence)\n",
    "\n",
    "    if not smoothing:\n",
    "        for key in TF.keys():\n",
    "            TF[key] /= length\n",
    "\n",
    "    else:\n",
    "        denom = 0\n",
    "        for key in TF.keys():\n",
    "            denom += (1 + math.log(TF[key]))\n",
    "\n",
    "        for key in TF.keys():\n",
    "            TF[key] /= denom\n",
    "\n",
    "    return TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a460c2b-f834-4d11-a4ef-37327fa1fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(sentence, sentences, smoothing=False):\n",
    "    IDF = {}\n",
    "    N = len(sentences)\n",
    "    for word in sentence:\n",
    "        IDF[word] = 0\n",
    "        for s in sentences:\n",
    "            if word in s:\n",
    "                IDF[word] += 1\n",
    "\n",
    "    if not smoothing:\n",
    "        for key in IDF.keys():\n",
    "            IDF[key] = math.log(N/IDF[key])\n",
    "\n",
    "    else:\n",
    "        for key in IDF.keys():\n",
    "            IDF[key] = math.log(((1+N)/(1+IDF[key]))) + 1\n",
    "\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8942d9a9-b642-4581-a76f-01bb96e8e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf_scores(sentences, smoothing=False):\n",
    "    TF_IDF = {}\n",
    "    for sentence in sentences:\n",
    "        List = []\n",
    "        TF = compute_tf_with_normalization(sentence, smoothing)\n",
    "        IDF = compute_idf(sentence, sentences, smoothing)\n",
    "        for key in TF.keys():\n",
    "            List.append(TF[key]*IDF[key])\n",
    "\n",
    "        TF_IDF[tuple(sentence)] = List\n",
    "\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0874f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preprocessed Sentences ===\n",
      "Sentence 1: ['apple', 'released', 'the', 'new', 'iphone', '<punct>', 'number', '<punct>', 'today', '<punct>', 'visit', '<punct>', 'url', '<punct>']\n",
      "Sentence 2: ['the', 'price', 'of', 'the', 'iphone', '<punct>', 'number', '<punct>', 'is', '<punct>', 'number', '<punct>', 'dollars', '<punct>']\n",
      "Sentence 3: ['check', '<punct>', 'url', '<punct>', 'for', 'more', 'details', '<punct>']\n",
      "\n",
      "=== TF-IDF Scores (per sentence) ===\n",
      "\n",
      "Sentence 1:\n",
      "apple           -> 0.078472\n",
      "released        -> 0.078472\n",
      "the             -> 0.028962\n",
      "new             -> 0.078472\n",
      "iphone          -> 0.028962\n",
      "<punct>         -> 0.000000\n",
      "number          -> 0.028962\n",
      "<punct>         -> 0.078472\n",
      "today           -> 0.078472\n",
      "<punct>         -> 0.028962\n",
      "\n",
      "Sentence 2:\n",
      "the             -> 0.057924\n",
      "price           -> 0.078472\n",
      "of              -> 0.078472\n",
      "the             -> 0.028962\n",
      "iphone          -> 0.000000\n",
      "<punct>         -> 0.057924\n",
      "number          -> 0.078472\n",
      "<punct>         -> 0.078472\n",
      "\n",
      "Sentence 3:\n",
      "check           -> 0.137327\n",
      "<punct>         -> 0.000000\n",
      "url             -> 0.050683\n",
      "<punct>         -> 0.137327\n",
      "for             -> 0.137327\n",
      "more            -> 0.137327\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    sentences = [\n",
    "        \"Apple released the new iPhone 15 today! Visit https://apple.com\",\n",
    "        \"The price of the iPhone 15 is 799 dollars.\",\n",
    "        \"Check www.example.com for more details.\"\n",
    "    ]\n",
    "\n",
    "    preprocessed_sentences = [preprocess(s) for s in sentences]\n",
    "\n",
    "    print(\"=== Preprocessed Sentences ===\")\n",
    "    for i, s in enumerate(preprocessed_sentences):\n",
    "        print(f\"Sentence {i+1}: {s}\")\n",
    "\n",
    "    tfidf_results = compute_tf_idf_scores(preprocessed_sentences, smoothing=False)\n",
    "\n",
    "    print(\"\\n=== TF-IDF Scores (per sentence) ===\")\n",
    "    for i, sentence_tokens in enumerate(preprocessed_sentences):\n",
    "        print(f\"\\nSentence {i+1}:\")\n",
    "        tfidf_list = tfidf_results[tuple(sentence_tokens)]\n",
    "        for token, score in zip(sentence_tokens, tfidf_list):\n",
    "            print(f\"{token:15} -> {score:.6f}\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9325a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "356450b2-01a6-4e84-8e08-3bc572973057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'boy', 'hugs', 'the', 'cat', '.'], ['the', 'boys', 'are', 'hugging', 'the', 'dogs', '.'], ['the', 'dogs', 'are', 'chasing', 'the', 'cats', '.'], ['the', 'dog', 'and', 'the', 'cat', 'sit', 'quietly', '.'], ['the', 'boy', 'is', 'sitting', 'on', 'the', 'dog', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The boy hugs the cat.\",\n",
    "    \"The boys are hugging the dogs.\",\n",
    "    \"The dogs are chasing the cats.\",\n",
    "    \"The dog and the cat sit quietly.\",\n",
    "    \"The boy is sitting on the dog.\"\n",
    "]\n",
    "\n",
    "def preprocess(sentences):\n",
    "    processed = []\n",
    "    for s in sentences:\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'([.,!?])', r' \\1 ', s)\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "        tokens = s.split()\n",
    "        processed.append(tokens)\n",
    "    return processed\n",
    "\n",
    "preprocessed_sentences = preprocess(sentences)\n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30246a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocab: Counter({'the##': 10, '.##': 5, 'boy##': 2, 'cat##': 2, 'are##': 2, 'dogs##': 2, 'dog##': 2, 'hugs##': 1, 'boys##': 1, 'hugging##': 1, 'chasing##': 1, 'cats##': 1, 'and##': 1, 'sit##': 1, 'quietly##': 1, 'is##': 1, 'sitting##': 1, 'on##': 1})\n"
     ]
    }
   ],
   "source": [
    "# Initialize vocab with all characters and a special end-of-word token ##\n",
    "vocab = Counter()\n",
    "\n",
    "# Split each word into characters + end-of-word marker (##)\n",
    "def get_initial_vocab(sentences):\n",
    "    words = []\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            chars = list(token)\n",
    "            chars.append('##')\n",
    "            words.append(chars)\n",
    "    return words\n",
    "\n",
    "word_sequences = get_initial_vocab(preprocessed_sentences)\n",
    "\n",
    "for seq in word_sequences:\n",
    "    vocab.update([''.join(seq)])\n",
    "\n",
    "print(\"Initial vocab:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc131375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top pairs: [(('e', '##'), 12), (('t', 'h'), 10), (('h', 'e'), 10), (('s', '##'), 6), (('.', '##'), 5), (('g', '##'), 5), (('d', 'o'), 4), (('o', 'g'), 4), (('b', 'o'), 3), (('o', 'y'), 3)]\n"
     ]
    }
   ],
   "source": [
    "def get_pair_counts(word_sequences):\n",
    "    pairs = Counter()\n",
    "    for seq in word_sequences:\n",
    "        for i in range(len(seq)-1):\n",
    "            pair = (seq[i], seq[i+1])\n",
    "            pairs[pair] += 1\n",
    "    return pairs\n",
    "\n",
    "pair_counts = get_pair_counts(word_sequences)\n",
    "print(\"Top pairs:\", pair_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "657b71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(pair_to_merge, word_sequences):\n",
    "    new_sequences = []\n",
    "    bigram = ''.join(pair_to_merge)\n",
    "    for seq in word_sequences:\n",
    "        i = 0\n",
    "        new_seq = []\n",
    "        while i < len(seq):\n",
    "            if i < len(seq)-1 and seq[i] == pair_to_merge[0] and seq[i+1] == pair_to_merge[1]:\n",
    "                new_seq.append(bigram)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_seq.append(seq[i])\n",
    "                i += 1\n",
    "        new_sequences.append(new_seq)\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f728ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final WordPiece vocabulary:\n",
      "['.##', 'and##', 'ar', 'are##', 'bo', 'boy', 'boy##', 'boys##', 'ca', 'cat', 'cat##', 'cats##', 'chasing##', 'do', 'dog##', 'dogs##', 'e##', 'g##', 'gs##', 'hu', 'hugging##', 'hugs##', 'in', 'ing##', 'is##', 'on##', 'quietly##', 's##', 'sit##', 'sitting##', 'th', 'the##']\n"
     ]
    }
   ],
   "source": [
    "num_merges = 20\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_pair_counts(word_sequences)\n",
    "    if not pairs:\n",
    "        break\n",
    "    most_freq_pair = pairs.most_common(1)[0][0]\n",
    "    word_sequences = merge_pair(most_freq_pair, word_sequences)\n",
    "    vocab[''.join(most_freq_pair)] += pairs[most_freq_pair]\n",
    "\n",
    "print(\"Final WordPiece vocabulary:\")\n",
    "print(sorted(vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d65d1652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['the##', 'cat##', 'is##', 'chasing##', 'the##', 'dog##', 'quietly##', '.##']\n"
     ]
    }
   ],
   "source": [
    "def wordpiece_tokenize(sentence, vocab):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'([.,!?])', r' \\1 ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    words = sentence.split()\n",
    "\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        chars = list(word) + ['##']\n",
    "        i = 0\n",
    "        current_tokens = []\n",
    "        while i < len(chars):\n",
    "            match = None\n",
    "            for j in range(len(chars), i, -1):\n",
    "                candidate = ''.join(chars[i:j])\n",
    "                if candidate in vocab:\n",
    "                    match = candidate\n",
    "                    break\n",
    "            if match:\n",
    "                current_tokens.append(match)\n",
    "                i += len(match)\n",
    "            else:\n",
    "                current_tokens.append(chars[i])\n",
    "                i += 1\n",
    "        tokens.extend(current_tokens)\n",
    "    return tokens\n",
    "\n",
    "test_sentence = \"The cat is chasing the dog quietly.\"\n",
    "tokenized_sentence = wordpiece_tokenize(test_sentence, vocab)\n",
    "print(\"Tokenized sentence:\", tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2c8b3ef-d522-4daa-8543-5adf76224d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NGram(Gram: int = 1, Paragraph: list = None, Smoothing = None):\n",
    "    L_List = []\n",
    "    G_List = []\n",
    "\n",
    "    if Paragraph is None:\n",
    "        return\n",
    "\n",
    "    # Function to flatten nested lists inside each sentence\n",
    "    def flatten(sentence):\n",
    "        flat = []\n",
    "        for w in sentence:\n",
    "            if isinstance(w, list):\n",
    "                flat.extend(w)   # add inner tokens\n",
    "            else:\n",
    "                flat.append(w)\n",
    "        return flat\n",
    "\n",
    "    # --- Unigrams ---\n",
    "    if Gram >= 1:\n",
    "        Gram_1 = {}\n",
    "        total = 0\n",
    "        for sentence in Paragraph:\n",
    "            sentence = flatten(sentence)   # ðŸŒŸ FIX APPLIED HERE\n",
    "            for word in sentence:\n",
    "                Gram_1[word] = Gram_1.get(word, 0) + 1\n",
    "                total += 1\n",
    "\n",
    "        Count_1 = copy.deepcopy(Gram_1)\n",
    "        for k in Gram_1:\n",
    "            Gram_1[k] = Gram_1[k] / total\n",
    "\n",
    "        L_List.append(Count_1)\n",
    "        G_List.append(Gram_1)\n",
    "\n",
    "    # --- Bigrams ---\n",
    "    if Gram >= 2:\n",
    "        Gram_2 = {}\n",
    "        for sentence in Paragraph:\n",
    "            sentence = flatten(sentence)   # ðŸŒŸ FIX APPLIED HERE TOO\n",
    "            for i in range(len(sentence)-1):\n",
    "                s = tuple(sentence[i:i+2])\n",
    "                Gram_2[s] = Gram_2.get(s, 0) + 1\n",
    "\n",
    "        Count_2 = copy.deepcopy(Gram_2)\n",
    "        for k in Gram_2:\n",
    "            Gram_2[k] = Gram_2[k] / Count_1.get(k[0], 1)\n",
    "\n",
    "        L_List.append(Count_2)\n",
    "        G_List.append(Gram_2)\n",
    "\n",
    "    # --- Trigrams ---\n",
    "    if Gram >= 3:\n",
    "        Gram_3 = {}\n",
    "        for sentence in Paragraph:\n",
    "            sentence = flatten(sentence)\n",
    "            for i in range(len(sentence)-2):\n",
    "                s = tuple(sentence[i:i+3])\n",
    "                Gram_3[s] = Gram_3.get(s, 0) + 1\n",
    "\n",
    "        Count_3 = copy.deepcopy(Gram_3)\n",
    "        for k in Gram_3:\n",
    "            Gram_3[k] = Gram_3[k] / Count_2.get(k[:2], 1)\n",
    "\n",
    "        L_List.append(Count_3)\n",
    "        G_List.append(Gram_3)\n",
    "\n",
    "    # --- Quadgrams ---\n",
    "    if Gram >= 4:\n",
    "        Gram_4 = {}\n",
    "        for sentence in Paragraph:\n",
    "            sentence = flatten(sentence)\n",
    "            for i in range(len(sentence)-3):\n",
    "                s = tuple(sentence[i:i+4])\n",
    "                Gram_4[s] = Gram_4.get(s, 0) + 1\n",
    "\n",
    "        Count_4 = copy.deepcopy(Gram_4)\n",
    "        for k in Gram_4:\n",
    "            Gram_4[k] = Gram_4[k] / Count_3.get(k[:3], 1)\n",
    "\n",
    "        L_List.append(Count_4)\n",
    "        G_List.append(Gram_4)\n",
    "\n",
    "    # --- Apply Smoothing if Provided ---\n",
    "    if Smoothing is not None:\n",
    "        L1, G1 = L_List[0], G_List[0]\n",
    "        L2 = L3 = L4 = None\n",
    "        G2 = G3 = G4 = None\n",
    "        if Gram >= 2:\n",
    "            L2, G2 = L_List[1], G_List[1]\n",
    "        if Gram >= 3:\n",
    "            L3, G3 = L_List[2], G_List[2]\n",
    "        if Gram >= 4:\n",
    "            L4, G4 = L_List[3], G_List[3]\n",
    "\n",
    "        Smoothing(Gram=Gram, L1=L1, L2=L2, L3=L3, L4=L4,\n",
    "                  G1=G1, G2=G2, G3=G3, G4=G4, Data=None)\n",
    "\n",
    "    return L_List, G_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78e8aad8-688d-4824-88c9-a5596a87e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddK_Smoothing(Gram=1, L1=None, L2=None, L3=None, L4=None, G1=None, G2=None, G3=None, G4=None, Data=None, K=0.3):\n",
    "\n",
    "    if Gram >= 1:\n",
    "        total_count = sum(L1.values())\n",
    "        V = len(L1)  # vocabulary size\n",
    "        for word in L1:\n",
    "            G1[word] = (L1[word] + K) / (total_count + K * V)\n",
    "\n",
    "    if Gram >= 2 and L2 is not None:\n",
    "        for bigram in L2:\n",
    "            history = bigram[0]\n",
    "            history_count = L1.get(history, 0)\n",
    "            G2[bigram] = (L2[bigram] + K) / (history_count + K * len(L1))\n",
    "\n",
    "    if Gram >= 3 and L3 is not None:\n",
    "        for trigram in L3:\n",
    "            history = trigram[:2]\n",
    "            history_count = L2.get(history, 0)\n",
    "            G3[trigram] = (L3[trigram] + K) / (history_count + K * len(L1))\n",
    "\n",
    "    if Gram >= 4 and L4 is not None:\n",
    "        for quadgram in L4:\n",
    "            history = quadgram[:3]\n",
    "            history_count = L3.get(history, 0)\n",
    "            G4[quadgram] = (L4[quadgram] + K) / (history_count + K * len(L1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1545cc6b-839b-4306-a82f-2ccb077320fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inform = [\"Check out https://example.com for more info!\", \"Your package #12345 will arrive tomorrow.\", \"Download the report from https://reports.com.\"]\n",
    "Reminder = [\"Meeting at 3pm, don't forget to bring the files.\", \"The meeting is starting in 10 minutes.\", \"Reminder: submit your timesheet by 5pm today.\"]\n",
    "Promo = [\"Order 3 items, get 1 free! Limited offer!!!\", \"Win $1000 now, visit http://winbig.com!!!\", \"Exclusive deal for you: buy 2, get 1 free!!!\"]\n",
    "\n",
    "Inform = [preprocess(s) for s in Inform]\n",
    "Reminder = [preprocess(s) for s in Reminder]\n",
    "Promo = [preprocess(s) for s in Promo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "874d703d-1d7f-4344-bd6d-9d1a9c876139",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inform_Count, Inform_Probability = NGram(2, Inform, AddK_Smoothing)\n",
    "Reminder_Count, Reminder_Probability = NGram(2, Reminder, AddK_Smoothing)\n",
    "Promo_Count, Promo_Probability = NGram(2, Promo, AddK_Smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9c85cf9-b27a-4302-83bc-0e86ae5b0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_probability(tokens, Count):\n",
    "    # Flatten any nested lists (IMPORTANT FIX)\n",
    "    flat = []\n",
    "    for t in tokens:\n",
    "        if isinstance(t, list):\n",
    "            flat.extend(t)\n",
    "        else:\n",
    "            flat.append(t)\n",
    "    tokens = flat  # overwrite\n",
    "\n",
    "    unigram_counts = Count[0]\n",
    "    bigram_counts = Count[1]\n",
    "\n",
    "    V = len(unigram_counts)\n",
    "    K = 1  # smoothing\n",
    "\n",
    "    prob = 1\n",
    "\n",
    "    for i in range(len(tokens)-1):\n",
    "        bg = (tokens[i], tokens[i+1])  # ensure tuple, not list\n",
    "\n",
    "        bg_count = bigram_counts.get(bg, 0)\n",
    "        history_count = unigram_counts.get(tokens[i], 0)\n",
    "\n",
    "        prob_bg = (bg_count + K) / (history_count + K * V)\n",
    "        prob *= prob_bg\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1b40743-8b1a-4688-a20c-e03b9a192a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"You will get an exclusive offer in the meeting!\"\n",
    "test_tokens = preprocess(test_sentence)\n",
    "\n",
    "prob_Inform = sentence_probability(test_tokens, Inform_Count)\n",
    "prob_Reminder = sentence_probability(test_tokens, Reminder_Count)\n",
    "prob_Promo = sentence_probability(test_tokens, Promo_Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27fb04f2-8ad2-43db-a6de-9da1649c3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inform: 9.691501262634235e-55 | Reminder: 7.623614053873445e-49 | Promotion: 4.102768402059395e-52\n",
      "Predicted class: Reminder\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inform: {prob_Inform} | Reminder: {prob_Reminder} | Promotion: {prob_Promo}\")\n",
    "categories = {\n",
    "    \"Inform\": prob_Inform,\n",
    "    \"Reminder\": prob_Reminder,\n",
    "    \"Promotion\": prob_Promo\n",
    "}\n",
    "predicted_label = max(categories.items(), key=lambda x: x[1])[0]\n",
    "print(f\"Predicted class: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
