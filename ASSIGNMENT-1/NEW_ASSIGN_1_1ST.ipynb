{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317ef3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kummarisamyuktha/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "#Load Telugu dataset\n",
    "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", name=\"indiccorp_v2\", split=\"tel_Telu\", streaming=True)\n",
    "\n",
    "# Small Telugu stopword list (expand if needed)\n",
    "telugu_stopwords = {\n",
    "    \"మరియు\", \"కాని\", \"లేక\", \"ఇది\", \"అది\", \"ఒక\",\n",
    "    \"లో\", \"పై\", \"తో\", \"కి\", \"వంటి\", \"అని\"\n",
    "}\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize\n",
    "    words = text.split(\" \")\n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in telugu_stopwords]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d792395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save cleaned dataset to file\n",
    "with open(\"telugu_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, example in enumerate(dataset):\n",
    "        if example[\"text\"].strip():\n",
    "            cleaned = clean_text(example[\"text\"])\n",
    "            if cleaned:\n",
    "                f.write(cleaned + \"\\n\")\n",
    "        # ⚠️ To avoid huge file, limit to first 50k sentences\n",
    "        if i >= 50000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d79e644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read back the file\n",
    "with open(\"telugu_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc1e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Splitting\n",
    "sentence_pattern = re.compile(r'(?<=[.!?])\\s+')\n",
    "all_sentences = sentence_pattern.split(text_data)\n",
    "no_of_sentences = len(all_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7533c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "token_pattern = re.compile(\n",
    "    r'\\bhttps?://\\S+|'                  # URLs\n",
    "    r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b|'  # email addresses\n",
    "    r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|'   # dates\n",
    "    r'\\d+\\.\\d+|\\d+|'                    # numbers\n",
    "    r'[\\u0C00-\\u0C7F]+|'                # Telugu words\n",
    "    r'[^\\s\\w\\u0C00-\\u0C7F]'             # punctuation\n",
    ")\n",
    "\n",
    "all_tokens = token_pattern.findall(text_data)\n",
    "no_of_words = len(all_tokens)\n",
    "unique_tokens = len(set(all_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17caa1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Character-level Analysis\n",
    "char_pattern = re.compile(r'[\\u0C00-\\u0C7F0-9.,!?;:\"\\'()\\[\\]{}\\-—–…\\s]')\n",
    "all_chars = char_pattern.findall(text_data)\n",
    "no_of_chars = len(all_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e75163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 85870\n",
      "Total words/tokens: 1114662\n",
      "Unique tokens: 130760\n",
      "Number of characters: 6827880\n",
      "Average word length: 6.125516075725198\n",
      "Average sentence length: 12.980808198439501\n",
      "TTR: 0.11730910356682116\n"
     ]
    }
   ],
   "source": [
    "Average_word_length = no_of_chars / no_of_words if no_of_words else 0\n",
    "Average_sentence_length = no_of_words / no_of_sentences if no_of_sentences else 0\n",
    "TTR = unique_tokens / no_of_words if no_of_words else 0\n",
    "\n",
    "print(\"Number of sentences:\", no_of_sentences)\n",
    "print(\"Total words/tokens:\", no_of_words)\n",
    "print(\"Unique tokens:\", unique_tokens)\n",
    "print(\"Number of characters:\", no_of_chars)\n",
    "print(\"Average word length:\", Average_word_length)\n",
    "print(\"Average sentence length:\", Average_sentence_length)\n",
    "print(\"TTR:\", TTR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
