{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Telugu dataset from telugu_dataset.txt...\n",
            "Dataset loaded successfully! Total paragraphs: 25,001\n"
          ]
        }
      ],
      "source": [
        "# ASSIGNMENT-1: Tokenized Sentences to Parquet Format\n",
        "# This notebook processes Telugu text and saves tokenized sentences to parquet format\n",
        "\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Load Telugu dataset from local file\n",
        "dataset_file = 'telugu_dataset.txt'\n",
        "print(f\"Loading Telugu dataset from {dataset_file}...\")\n",
        "\n",
        "# Read all paragraphs from the file (each line is a paragraph)\n",
        "with open(dataset_file, 'r', encoding='utf-8') as f:\n",
        "    paragraphs = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(f\"Dataset loaded successfully! Total paragraphs: {len(paragraphs):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization patterns defined!\n"
          ]
        }
      ],
      "source": [
        "# Define tokenization patterns\n",
        "\n",
        "# Sentence tokenization pattern - splits on punctuation followed by whitespace\n",
        "sentence_pattern = re.compile(r'(?<=[.!?])\\s+')\n",
        "\n",
        "# Word tokenization pattern - handles URLs, emails, dates, numbers, Telugu script, English, punctuation\n",
        "token_pattern = re.compile(\n",
        "    r'\\bhttps?://\\S+|'                  # URLs\n",
        "    r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,4}\\b|'  # email addresses\n",
        "    r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|'   # dates (DD/MM/YYYY or DD-MM-YYYY)\n",
        "    r'\\d+\\.\\d+|\\d+|'                    # numbers (decimals and integers)\n",
        "    r'[\\u0C00-\\u0C7F]+|'                # Telugu script\n",
        "    r'[^\\s\\w\\u0C00-\\u0C7F]'             # punctuation\n",
        ")\n",
        "\n",
        "def tokenize_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Tokenize a sentence into words/tokens and return as space-separated string.\n",
        "    \n",
        "    Args:\n",
        "        sentence: Input sentence string\n",
        "        \n",
        "    Returns:\n",
        "        Space-separated tokenized sentence\n",
        "    \"\"\"\n",
        "    tokens = token_pattern.findall(sentence)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"Tokenization patterns defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing dataset...\n",
            "Limit: 50,000 paragraphs\n",
            "============================================================\n",
            "Processed 1,000 paragraphs, 3,659 tokenized sentences\n",
            "Processed 2,000 paragraphs, 7,246 tokenized sentences\n",
            "Processed 3,000 paragraphs, 10,905 tokenized sentences\n",
            "Processed 4,000 paragraphs, 14,838 tokenized sentences\n",
            "Processed 5,000 paragraphs, 18,345 tokenized sentences\n",
            "Processed 6,000 paragraphs, 22,270 tokenized sentences\n",
            "Processed 7,000 paragraphs, 26,858 tokenized sentences\n",
            "Processed 8,000 paragraphs, 30,450 tokenized sentences\n",
            "Processed 9,000 paragraphs, 33,828 tokenized sentences\n",
            "Processed 10,000 paragraphs, 37,437 tokenized sentences\n",
            "Processed 11,000 paragraphs, 41,449 tokenized sentences\n",
            "Processed 12,000 paragraphs, 44,698 tokenized sentences\n",
            "Processed 13,000 paragraphs, 48,345 tokenized sentences\n",
            "Processed 14,000 paragraphs, 52,187 tokenized sentences\n",
            "Processed 15,000 paragraphs, 55,816 tokenized sentences\n",
            "Processed 16,000 paragraphs, 59,375 tokenized sentences\n",
            "Processed 17,000 paragraphs, 63,084 tokenized sentences\n",
            "Processed 18,000 paragraphs, 66,599 tokenized sentences\n",
            "Processed 19,000 paragraphs, 70,291 tokenized sentences\n",
            "Processed 20,000 paragraphs, 73,722 tokenized sentences\n",
            "Processed 21,000 paragraphs, 77,160 tokenized sentences\n",
            "Processed 22,000 paragraphs, 80,927 tokenized sentences\n",
            "Processed 23,000 paragraphs, 84,593 tokenized sentences\n",
            "Processed 24,000 paragraphs, 87,969 tokenized sentences\n",
            "Processed 25,000 paragraphs, 91,624 tokenized sentences\n",
            "Processed 26,000 paragraphs, 95,402 tokenized sentences\n",
            "Processed 27,000 paragraphs, 98,858 tokenized sentences\n",
            "Processed 28,000 paragraphs, 102,239 tokenized sentences\n",
            "Processed 29,000 paragraphs, 105,751 tokenized sentences\n",
            "Processed 30,000 paragraphs, 109,391 tokenized sentences\n",
            "Processed 31,000 paragraphs, 113,086 tokenized sentences\n",
            "Processed 32,000 paragraphs, 116,736 tokenized sentences\n",
            "Processed 33,000 paragraphs, 120,555 tokenized sentences\n",
            "Processed 34,000 paragraphs, 124,116 tokenized sentences\n",
            "Processed 35,000 paragraphs, 127,797 tokenized sentences\n",
            "Processed 36,000 paragraphs, 131,455 tokenized sentences\n",
            "Processed 37,000 paragraphs, 134,903 tokenized sentences\n",
            "Processed 38,000 paragraphs, 138,563 tokenized sentences\n",
            "Processed 39,000 paragraphs, 142,275 tokenized sentences\n",
            "Processed 40,000 paragraphs, 145,894 tokenized sentences\n",
            "Processed 41,000 paragraphs, 149,514 tokenized sentences\n",
            "Processed 42,000 paragraphs, 153,362 tokenized sentences\n",
            "Processed 43,000 paragraphs, 156,828 tokenized sentences\n",
            "Processed 44,000 paragraphs, 160,396 tokenized sentences\n",
            "Processed 45,000 paragraphs, 164,151 tokenized sentences\n",
            "Processed 46,000 paragraphs, 167,789 tokenized sentences\n",
            "Processed 47,000 paragraphs, 171,442 tokenized sentences\n",
            "Processed 48,000 paragraphs, 175,285 tokenized sentences\n",
            "Processed 49,000 paragraphs, 178,680 tokenized sentences\n",
            "Processed 50,000 paragraphs, 182,239 tokenized sentences\n",
            "\n",
            "Reached limit of 50,000 paragraphs\n",
            "============================================================\n",
            "\n",
            "Processing complete!\n",
            "Total paragraphs processed: 50,000\n",
            "Total tokenized sentences: 182,239\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION: Set processing limit\n",
        "# ============================================================================\n",
        "# Change MAX_PARAGRAPHS to control how many paragraphs to process:\n",
        "#   - None: Process entire dataset\n",
        "#   - 1000: Quick test (~1-2 minutes)\n",
        "#   - 10000: Small sample (~5-10 minutes)\n",
        "#   - 25000: Full dataset (based on telugu_dataset.txt size)\n",
        "# ============================================================================\n",
        "MAX_PARAGRAPHS = None  # Set to None to process all, or a number to limit\n",
        "\n",
        "# Process dataset: Split paragraphs into sentences, tokenize each sentence, and collect results\n",
        "\n",
        "tokenized_sentences = []\n",
        "num_paragraphs = 0\n",
        "batch_size = 1000  # Process in batches for progress tracking\n",
        "\n",
        "# Determine how many paragraphs to process\n",
        "total_paragraphs = len(paragraphs)\n",
        "if MAX_PARAGRAPHS:\n",
        "    paragraphs_to_process = paragraphs[:MAX_PARAGRAPHS]\n",
        "    print(f\"Processing {MAX_PARAGRAPHS:,} paragraphs (out of {total_paragraphs:,} total)\")\n",
        "else:\n",
        "    paragraphs_to_process = paragraphs\n",
        "    print(f\"Processing entire dataset: {total_paragraphs:,} paragraphs\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for paragraph in paragraphs_to_process:\n",
        "    if paragraph.strip():\n",
        "        # Step 1: Split paragraph into sentences\n",
        "        sentences = sentence_pattern.split(paragraph)\n",
        "        \n",
        "        # Step 2: Tokenize each sentence and join tokens with spaces\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.strip()\n",
        "            if sentence:  # Skip empty sentences\n",
        "                tokenized = tokenize_sentence(sentence)\n",
        "                if tokenized:  # Only add non-empty tokenized sentences\n",
        "                    tokenized_sentences.append(tokenized)\n",
        "        \n",
        "        num_paragraphs += 1\n",
        "        \n",
        "        # Progress update\n",
        "        if num_paragraphs % batch_size == 0:\n",
        "            print(f\"Processed {num_paragraphs:,} paragraphs, {len(tokenized_sentences):,} tokenized sentences\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nProcessing complete!\")\n",
        "print(f\"Total paragraphs processed: {num_paragraphs:,}\")\n",
        "print(f\"Total tokenized sentences: {len(tokenized_sentences):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving to telugu_tokenized_sentences.parquet...\n",
            "✓ Successfully saved 182239 tokenized sentences\n",
            "✓ File: telugu_tokenized_sentences.parquet\n",
            "✓ File size: 14.92 MB\n",
            "✓ Compression: Snappy\n"
          ]
        }
      ],
      "source": [
        "# Create DataFrame and save to Parquet format with compression\n",
        "\n",
        "# Create DataFrame with tokenized sentences\n",
        "df = pd.DataFrame({\n",
        "    'tokenized_sentence': tokenized_sentences\n",
        "})\n",
        "\n",
        "# Output file name\n",
        "output_file = 'telugu_tokenized_sentences.parquet'\n",
        "\n",
        "print(f\"Saving to {output_file}...\")\n",
        "\n",
        "# Save to parquet with compression\n",
        "df.to_parquet(\n",
        "    output_file,\n",
        "    engine='pyarrow',           # Use PyArrow engine for better performance\n",
        "    compression='snappy',       # Snappy compression: fast, good compression ratio\n",
        "    index=False                 # Don't save DataFrame index\n",
        ")\n",
        "\n",
        "# Get file size\n",
        "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "\n",
        "print(f\"✓ Successfully saved {len(tokenized_sentences)} tokenized sentences\")\n",
        "print(f\"✓ File: {output_file}\")\n",
        "print(f\"✓ File size: {file_size_mb:.2f} MB\")\n",
        "print(f\"✓ Compression: Snappy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying parquet file...\n",
            "============================================================\n",
            "Total sentences in parquet file: 182239\n",
            "\n",
            "First 10 tokenized sentences:\n",
            "------------------------------------------------------------\n",
            "1. అమెరికా అధ్యక్షుడు డొనాల్డ్ ట్రంప్ కు రాష్ట్రపతి భవన్ వద్ద ఘనస్వాగతం లభించింది ....\n",
            "2. ఆయనకు రాష్ట్రపతి రామ్ నాథ్ కోవింద్ దంపతులు , ప్రధాని మోదీ సాదరంగా ఆహ్వానం పలకడంతో పాటు సైనికులు గౌరవ...\n",
            "3. ఇటు తెలంగాణలో కరోనా వైరస్ కారణంగా అన్ని దేవాలయాల్లో ముందస్తు చర్యలు చేపట్టారు ....\n",
            "4. భద్రాద్రి రాముడికి కరోనా ఎఫెక్ట్ తగిలింది ....\n",
            "5. ఏప్రిల్ 2 న భద్రాద్రిలో జరగనున్న శ్రీరామ నవమి వేడుకలను వెంటాడుతోంది కరోనా ....\n",
            "6. రాష్ట్రంలో కొనసాగుతున్న కరోనా అలర్ట్ నేపథ్యంలో భక్తులు లేకుండానే శ్రీరామనవమి జరుపుతామని మంత్రి పువ్వ...\n",
            "7. ప్రత్యేక మీడియా సమావేశం ఏర్పాటు చేసిన మంత్రి పువ్వాడ ఈ మేరకు స్పష్టం చేశారు ....\n",
            "8. శ్రీ రాములవారి కల్యాణం టికెట్లు రద్దు చేస్తున్నామని ప్రకటించారు . ....\n",
            "9. టికెట్ ‌ డబ్బు తిరిగి ఆలయ అధికారులు చెల్లిస్తారన్నారు ....\n",
            "10. కరోనాపై ప్రజలు భయభ్రాంతులకు గురికావొదని , మరింత అప్రమత్తంగా ఉండాలని ఉండాలని సూచించారు ....\n",
            "\n",
            "============================================================\n",
            "✓ Parquet file verified successfully!\n"
          ]
        }
      ],
      "source": [
        "# Verify the parquet file - Read back and display sample\n",
        "\n",
        "print(\"Verifying parquet file...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Read the parquet file\n",
        "df_read = pd.read_parquet(output_file)\n",
        "\n",
        "print(f\"Total sentences in parquet file: {len(df_read)}\")\n",
        "print(f\"\\nFirst 10 tokenized sentences:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for idx, row in df_read.head(10).iterrows():\n",
        "    print(f\"{idx + 1}. {row['tokenized_sentence'][:100]}...\")  # Show first 100 chars\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ Parquet file verified successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus Statistics:\n",
            "============================================================\n",
            "Total sentences: 182,239\n",
            "Total tokens: 2,242,620\n",
            "Unique tokens: 200,756\n",
            "Average sentence length: 12.31 tokens\n",
            "Type-Token Ratio (TTR): 0.0895\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Optional: Calculate and display statistics\n",
        "\n",
        "if len(tokenized_sentences) > 0:\n",
        "    # Calculate statistics\n",
        "    all_tokens = []\n",
        "    for sent in tokenized_sentences:\n",
        "        all_tokens.extend(sent.split())\n",
        "    \n",
        "    total_tokens = len(all_tokens)\n",
        "    unique_tokens = len(set(all_tokens))\n",
        "    avg_sentence_length = total_tokens / len(tokenized_sentences) if tokenized_sentences else 0\n",
        "    ttr = unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "    \n",
        "    print(\"Corpus Statistics:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Total sentences: {len(tokenized_sentences):,}\")\n",
        "    print(f\"Total tokens: {total_tokens:,}\")\n",
        "    print(f\"Unique tokens: {unique_tokens:,}\")\n",
        "    print(f\"Average sentence length: {avg_sentence_length:.2f} tokens\")\n",
        "    print(f\"Type-Token Ratio (TTR): {ttr:.4f}\")\n",
        "    print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
