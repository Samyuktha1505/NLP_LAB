{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d965970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_telugu_corpus():\n",
    "    # Auto-handle broken UTF-8 characters\n",
    "    with open(\"../ASSIGNMENT-1/telugu_dataset.txt\", \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return lines\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    return text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704008d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class FastBPE:\n",
    "    def __init__(self):\n",
    "        self.merges = []\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, corpus, num_merges=32000):\n",
    "        word_freqs = defaultdict(int)\n",
    "        for line in corpus:\n",
    "            for word in whitespace_tokenize(line):\n",
    "                chars = tuple(list(word) + [\"</w>\"])\n",
    "                word_freqs[chars] += 1\n",
    "\n",
    "        for w in word_freqs:\n",
    "            for ch in w:\n",
    "                self.vocab.add(ch)\n",
    "\n",
    "        pair_stats = Counter()\n",
    "        word_instances = defaultdict(list)\n",
    "\n",
    "        # Track all pair occurrences initially\n",
    "        for word, freq in word_freqs.items():\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i+1])\n",
    "                pair_stats[pair] += freq\n",
    "                word_instances[pair].append((word, i))\n",
    "\n",
    "        # Main merge loop\n",
    "        for _ in range(num_merges):\n",
    "            if not pair_stats:\n",
    "                break\n",
    "\n",
    "            best = pair_stats.most_common(1)[0][0]\n",
    "            self.merges.append(best)\n",
    "\n",
    "            occurrences = word_instances.pop(best, [])\n",
    "            if not occurrences:\n",
    "                continue\n",
    "\n",
    "            new_pair_stats = Counter()\n",
    "\n",
    "            for word, pos in occurrences:\n",
    "                if word not in word_freqs:\n",
    "                    continue\n",
    "\n",
    "                freq = word_freqs[word]\n",
    "\n",
    "                # Merge operation\n",
    "                merged = []\n",
    "                i = 0\n",
    "                L = len(word)\n",
    "                while i < L:\n",
    "                    if i < L - 1 and (word[i], word[i+1]) == best:\n",
    "                        merged.append(word[i] + word[i+1])\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        merged.append(word[i])\n",
    "                        i += 1\n",
    "                merged = tuple(merged)\n",
    "\n",
    "                # Update frequencies\n",
    "                del word_freqs[word]\n",
    "                word_freqs[merged] += freq\n",
    "\n",
    "                # Update pair stats for newly created word\n",
    "                for i in range(len(merged) - 1):\n",
    "                    pair = (merged[i], merged[i+1])\n",
    "                    new_pair_stats[pair] += freq\n",
    "                    word_instances[pair].append((merged, i))\n",
    "\n",
    "            pair_stats = new_pair_stats\n",
    "\n",
    "        for w in word_freqs:\n",
    "            for ch in w:\n",
    "                self.vocab.add(ch)\n",
    "\n",
    "    def encode_word(self, word):\n",
    "        tokens = tuple(list(word) + [\"</w>\"])\n",
    "        for pair in self.merges:\n",
    "            merged = []\n",
    "            i = 0\n",
    "            while i < len(tokens):\n",
    "                if i < len(tokens)-1 and tokens[i:i+2] == pair:\n",
    "                    merged.append(tokens[i] + tokens[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    merged.append(tokens[i])\n",
    "                    i += 1\n",
    "            tokens = tuple(merged)\n",
    "        if tokens[-1] == \"</w>\":\n",
    "            tokens = tokens[:-1]\n",
    "        return list(tokens)\n",
    "\n",
    "    def encode(self, text):\n",
    "        out = []\n",
    "        for w in whitespace_tokenize(text):\n",
    "            out.extend(self.encode_word(w))\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c2c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class WordPiece:\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.vocab_list = []\n",
    "        self.unk = \"[UNK]\"\n",
    "\n",
    "    def tokenize_word(self, word):\n",
    "        start = 0\n",
    "        sub_tokens = []\n",
    "\n",
    "        while start < len(word):\n",
    "            end = len(word)\n",
    "            cur = None\n",
    "\n",
    "            while start < end:\n",
    "                piece = word[start:end]\n",
    "                if start > 0:\n",
    "                    piece = \"##\" + piece\n",
    "\n",
    "                if piece in self.vocab:\n",
    "                    cur = piece\n",
    "                    break\n",
    "                end -= 1\n",
    "\n",
    "            if not cur:\n",
    "                return [self.unk]\n",
    "\n",
    "            sub_tokens.append(cur)\n",
    "            start = end if not cur.startswith(\"##\") else start + len(cur) - 2\n",
    "\n",
    "        return sub_tokens\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        result = []\n",
    "        for w in whitespace_tokenize(text):\n",
    "            result.extend(self.tokenize_word(w))\n",
    "        return result\n",
    "\n",
    "    def train(self, corpus, vocab_size=32000):\n",
    "        word_freq = defaultdict(int)\n",
    "        for line in corpus:\n",
    "            for w in whitespace_tokenize(line):\n",
    "                word_freq[w] += 1\n",
    "\n",
    "        chars = set()\n",
    "        for w in word_freq:\n",
    "            for ch in w:\n",
    "                chars.add(ch)\n",
    "\n",
    "        self.vocab = set(chars)\n",
    "        self.vocab.add(self.unk)\n",
    "        self.vocab_list = [self.unk] + sorted(chars)\n",
    "\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            pair_freqs = defaultdict(int)\n",
    "\n",
    "            for w, freq in word_freq.items():\n",
    "                toks = self.tokenize_word(w)\n",
    "                for i in range(len(toks)-1):\n",
    "                    pair = (toks[i], toks[i+1])\n",
    "                    pair_freqs[pair] += freq\n",
    "\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "\n",
    "            best = max(pair_freqs, key=pair_freqs.get)\n",
    "            t1, t2 = best\n",
    "\n",
    "            def raw(x): return x[2:] if x.startswith(\"##\") else x\n",
    "\n",
    "            if t1.startswith(\"##\"):\n",
    "                new_tok = \"##\" + raw(t1) + raw(t2)\n",
    "            else:\n",
    "                new_tok = raw(t1) + raw(t2)\n",
    "\n",
    "            if new_tok in self.vocab:\n",
    "                continue\n",
    "\n",
    "            self.vocab.add(new_tok)\n",
    "            self.vocab_list.append(new_tok)\n",
    "\n",
    "            if len(self.vocab) >= vocab_size:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac04ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training FAST BPE... this will be MUCH faster...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    corpus = load_telugu_corpus()\n",
    "\n",
    "    # -------- BPE ----------\n",
    "    print(\"\\nTraining FAST BPE... this will be MUCH faster...\")\n",
    "    bpe = FastBPE()\n",
    "    bpe.train(corpus, num_merges=32000)\n",
    "    print(\"BPE vocab size:\", len(bpe.vocab))\n",
    "    print(\"BPE sample:\", bpe.encode(\"నేను మంచి పిల్లను\"))\n",
    "\n",
    "    # -------- WordPiece ----------\n",
    "    print(\"\\nTraining WordPiece...\")\n",
    "    wp = WordPiece()\n",
    "    wp.train(corpus, vocab_size=32000)\n",
    "    print(\"WordPiece vocab:\", len(wp.vocab))\n",
    "    print(\"WP sample:\", wp.tokenize(\"నేను మంచి పిల్లను\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c872467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
